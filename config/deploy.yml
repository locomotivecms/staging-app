# Name of your application. Used to uniquely configure containers.
service: staging-app

# Name of the container image.
image: locomotivecms/staging-app

# Deploy to these servers.
servers:  
  web:
    hosts:
      - 174.138.51.62
    labels:
      traefik.http.routers.staging-app.entrypoints: websecure
      traefik.http.routers.staging-app.rule: Host(`beta2.locomotive.works`)
      traefik.http.routers.staging-app.tls.certresolver: letsencrypt
    options:
      network: "private"

# Credentials for your image host.
registry:
  server: ghcr.io
  username: did
  password:
    - DOCKER_REGISTRY_TOKEN

accessories:
  mongodb:
    image: mongodb/mongodb-community-server:7.0.12
    roles:
      - web
    env:
      secret:
        - MONGO_INITDB_ROOT_USERNAME
        - MONGO_INITDB_ROOT_PASSWORD
    volumes:
      - /var/lib/mongodb:/data/db
    options:
      network: "private"

  redis:
    image: redis:7.0
    roles:
      - web
    volumes:
      - /var/lib/redis:/data
    options:
      network: "private" 

# Inject ENV variables into containers (secrets come from .env).
# Remember to run `kamal env push` after making changes!
env:
  clear:
    RAILS_SERVE_STATIC_FILES: true
    RAILS_LOG_TO_STDOUT: true
    LANG: en_US.UTF-8
    NEW_RELIC_LOG: stdout
    RACK_ENV: production
    RAILS_ENV: production
    RAILS_LOG_TO_STDOUT: enabled
    RAILS_SERVE_STATIC_FILES: enabled 
    REDIS_URL: "redis://staging-app-redis:6379/0"
  secret:
    - RAILS_MASTER_KEY
    - SECRET_KEY_BASE
    - DRAGONFLYAPP_URL
    - DRAGONFLY_SECRET_KEY
    - MONGODB_URI
    - NEW_RELIC_LICENSE_KEY
    - S3_ASSET_HOST_URL
    - S3_BUCKET
    - S3_BUCKET_REGION
    - S3_KEY_ID
    - S3_SECRET_KEY
    - SCOUT_APM_KEY
    - SENDGRID_USERNAME
    - SENDGRID_PASSWORD
    - MONGO_INITDB_ROOT_USERNAME
    - MONGO_INITDB_ROOT_PASSWORD

# Use a different ssh user than root
# ssh:
#   user: app

# Configure builder setup.
# builder:
#   args:
#     RUBY_VERSION: 3.2.0
#   secrets:
#     - GITHUB_TOKEN
#   remote:
#     arch: amd64
#     host: ssh://app@192.168.0.1

# Use accessory services (secrets come from .env).
# accessories:
#   db:
#     image: mysql:8.0
#     host: 192.168.0.2
#     port: 3306
#     env:
#       clear:
#         MYSQL_ROOT_HOST: '%'
#       secret:
#         - MYSQL_ROOT_PASSWORD
#     files:
#       - config/mysql/production.cnf:/etc/mysql/my.cnf
#       - db/production.sql:/docker-entrypoint-initdb.d/setup.sql
#     directories:
#       - data:/var/lib/mysql
#   redis:
#     image: redis:7.0
#     host: 192.168.0.2
#     port: 6379
#     directories:
#       - data:/data

# Configure custom arguments for Traefik. Be sure to reboot traefik when you modify it.
# traefik:
#   args:
#     accesslog: true
#     accesslog.format: json

traefik:
  options:
    publish:
      - "443:443"
    volume:
      - "/letsencrypt/acme.json:/letsencrypt/acme.json" # To save the configuration file.
    network: "private"
  args:
    entryPoints.web.address: ":80"
    entryPoints.websecure.address: ":443"
    entryPoints.web.http.redirections.entryPoint.to: websecure # We want to force https
    entryPoints.web.http.redirections.entryPoint.scheme: https
    entryPoints.web.http.redirections.entrypoint.permanent: true
    certificatesResolvers.letsencrypt.acme.email: "didier@nocoffee.fr"
    certificatesResolvers.letsencrypt.acme.storage: "/letsencrypt/acme.json" # Must match the path in `volume`
    certificatesResolvers.letsencrypt.acme.httpchallenge: true
    certificatesResolvers.letsencrypt.acme.httpchallenge.entrypoint: web

# Configure a custom healthcheck (default is /up on port 3000)
healthcheck:
  # path: /healthz
  # port: 4000
  interval: 5s

# Bridge fingerprinted assets, like JS and CSS, between versions to avoid
# hitting 404 on in-flight requests. Combines all files from new and old
# version inside the asset_path.
#
# If your app is using the Sprockets gem, ensure it sets `config.assets.manifest`.
# See https://github.com/basecamp/kamal/issues/626 for details
#
# asset_path: /rails/public/assets

# Configure rolling deploys by setting a wait time between batches of restarts.
# boot:
#   limit: 10 # Can also specify as a percentage of total hosts, such as "25%"
#   wait: 2

# Configure the role used to determine the primary_host. This host takes
# deploy locks, runs health checks during the deploy, and follow logs, etc.
#
# Caution: there's no support for role renaming yet, so be careful to cleanup
#          the previous role on the deployed hosts.
# primary_role: web

# Controls if we abort when see a role with no hosts. Disabling this may be
# useful for more complex deploy configurations.
#
# allow_empty_roles: false
